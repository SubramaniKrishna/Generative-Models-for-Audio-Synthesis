{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Modelling Synthesis\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Audio Synthesis\n",
    "***\n",
    "[Link to Audio Examples](https://chrisdonahue.com/wavegan_examples/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WaveGAN** - A first attempt to synthesize audio from raw time-domain waveforms  \n",
    "- Unlike previous naive attempts to simply bootstrap algos by treating spectrograms as images. This offers better performance\n",
    "- Qualitative(Human Judgement) + Quantitative(Inception Score) evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical application in generating sounds - Artist can explore **latent space** of audio, and fine tune variables/parameters to generate desired sound, as opposed to finding his desired sound from dataset. However, audio has high temporal resolution, and thus latent space should encode these high dimensions correctly.  \n",
    "Other work - \n",
    "1. Autoregressive models(Wavenet) -> *Very* slow generation of audio\n",
    "2. GAN's used naively on image spectrograms -> Lossy estimates due to non-invertibility of spectrogram, thus learn an inversion model as well  \n",
    "The authors want to investigate if unsupervised strategies can learn **semantic nodes** implicitly in the high dimensional space rather than being conditioned on them<Discuss more?!>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper - \n",
    "1. Waveform(WaveGAN) + Spectrogram(SpecGAN) strategies for GAN's\n",
    "2. Human Evaluation for sounds + Quantitative evaluation on Speech dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>WaveGAN</u>  \n",
    "Images vs Audio - \n",
    "    - Audio more likely to exhibit periodic structure\n",
    "    - Correlations across large time instants in audio(-> Filter with larger RF!)\n",
    "\n",
    "Architecture - \n",
    "- Modification of Deep Conv GAN\n",
    "    1. Flattened(1D instead of 2D) convolutions\n",
    "    2. Increase stride\n",
    "    3. Rather than original GAN cost(which is unstable due to non-differentiability), use WGAN(modified stable cost function)\n",
    "- Phase Shuffling(artifact prevention, phase invariance)\n",
    "\n",
    "<u>SpecGAN</u>  \n",
    "Phase information is often discarded which prevents inversion of spectrogram\n",
    "\n",
    "Architecture - \n",
    "- Audio ---- [STFT] -> TF Representation ---- [Train DCGAN] -> Obtain samples ---- [Griffin Lim for phase reconstruction] -> Obtain audio from samples\n",
    "\n",
    "Poor performance due to noisiness introduced in Griffin Lim inversion process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Dataset</u>  \n",
    "1. Speech(SC09)\n",
    "2. Sounds(Drum, Bird, Piano, Large Vocab Speech)\n",
    "\n",
    "**Took ~4 days to train!**\n",
    "\n",
    "<u>Evaluation</u>  \n",
    "1. Inception Score - $e^{E_{x}KL(P(y/x)||P(y))}$\n",
    "    - P(y/x) should be low entropy(deterministic){Data generated given class label}\n",
    "    - P(y) should be high entropy(uniform){Data generated across all labels}\n",
    "2. NN comparison - Correct for errors in inception score\n",
    "3. Qualitative Human Judgement - **Amazon Mechanical Turk** to collect/label audio. Digit perceived(SC09) + Sound quality(0-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future Work - \n",
    "- Variable length audio\n",
    "- label conditioning strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Adversarial Generation of Time-Frequency Features with application in audio synthesis\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STFT as Time Frequency(TF) Representation of audio i.e. GAN trained on STFT features. This outperforms models trained on waveform directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Phase of STFT hard to understand and model => Use partial derivatives of phase(local instantaneous frequency).\n",
    "- Phase estimation from magnitude spectrogram(Griffin Lim) unreliable\n",
    "- Inspired from Phaseless Reconstruction + Current State of Art i.e. Gansynth -> TiFGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Math</u>  \n",
    "- STFT maps time-domain signals in to a lower-dimensional subspace of all possible magnitudes\n",
    "- Look at STFT as an operator, and study the conditions for the operator to be perfectly invertible.\n",
    "- For a consistent transformation, assuming that STFT is an analytic function, using the Cauchy Riemann equations, you get a coupled pair of equations which can be solved to obtain the phase simply from the log-Magnitude Spectrogram itself.\n",
    "- Use of **Phase Gradient Heap Integration(PGHI)** to bypass phase instabilitites by providing betted estimates.\n",
    "- The above condition is discretized, and using a new metric **consistency**, the authors judge goodness of TF representation. Consistency evaluated by looking ate projection error $\\hat{e} = |S^{gen} - S^{proj}|^{2}$ where $S^{proj} = ISTFT(STFT(S))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture\n",
    "- Modified DCGAN + preprocessing on signal to enable its input to a GAN\n",
    "\n",
    "Evaluation\n",
    "- Dataset - \n",
    "1) Speech commands dataset\n",
    "2) MUSIC, 25 min of BACH piano recordings\n",
    "- Evaluation using Inception Score and Frechet inception distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion, Future work  -  \n",
    "1. Consistency measure - Computationally cheap measure to assess quality of TF representation  \n",
    "2. Extension - Use logarithmic and perceptual frequency scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### GANSYNTH: ADVERSARIAL NEURAL AUDIO SYNTHESIS(State of the Art!!!)\n",
    "***\n",
    "[Audio Examples](https://storage.googleapis.com/magentadata/papers/gansynth/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human perception of audio sensitive to both **global structure** and **fine scale waveform coherence**.\n",
    "1. Autoregressive models(Wavenet, Autoencoder Wavenet) capture fine scale waveform, but lack global latent structure\n",
    "2. GAN's have **global latent conditioning** and **efficient parallel sampling**, but lack local coherence.\n",
    "\n",
    "This paper demonstrates that GAN's can indeed generate **high-fidelity** and **locally coherent** waveforms by modelling **log magnitudes** and **Instantaneuos Frequencies** in Spectral Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive models - \n",
    "    - Focusing on 'finest scale' i.e. a sample\n",
    "    - rely on **external conditioning** for global structure\n",
    "    - Sampling very slow(**ancestral sampling**) as generate waveform one sample at a time\n",
    "    - Due to **fine timescale**, autoencoder variants model only local latent structure.\n",
    "GAN's - \n",
    "    - Stack of transposed convolutions on latent vector\n",
    "    - lack perceptual fidelity of image counterparts\n",
    "\n",
    "Motivation for using phase - \n",
    "- Problem of phase precession when frame size not equal to period(also for overlapping filterbanks)\n",
    "![phase_precession](fig_01.PNG)\n",
    "- Human perception highly sensitive to doscontinuities and irregularities in periodic waveforms\n",
    "- Challenge for synthesis network - It must learn all the correct frequency+phase combinations to output a coherent waveform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper - \n",
    "1. Generate log-magnitude spectrograms and phases directly with GAN\n",
    "2. **Estimate Instantaneuos Frequency Spectra(Rather than Phase), more coherent audio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Dataset</u> - \n",
    "- Nsynth(available online)\n",
    "- restricted to training on subsets of accoustic instruemtns, and limited pitch range(MIDI 24-84 ~ 1000Hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Architecture</u> - \n",
    "- Progressive training of GAN's(Karras 2018a paper)\n",
    "- Condition on additional source of information(pitch) to achieve independent control of pitch and timbre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Evaluation</u> - \n",
    "1. Human Evaluation\n",
    "2. Number of Statistically Different Bins(NDB) - measure diversity of generated samples\n",
    "3. Inception Score\n",
    "4. Pitch accuracy, pitch entropy\n",
    "5. Frechet Inception Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Discussion</u> - \n",
    "- Phase Coherence of generated waveforms(Rainbowgrams!)\n",
    "![coherence_gen_wf](fig_02.PNG)\n",
    "- Interpolation(**spherical interpolation** vs linear interpolation in previous work) observed smooth perceptual changes, no major artifacts\n",
    "- Consistent Timbre across pitch, fix latent vector and condition on pitch. Timbral identity is constant for given point in latent space.\n",
    "- Extremely fast generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future Work - \n",
    "- Combining adversarial losses with encoders(use VAE to model G and D)\n",
    "- More straightforward **regression losses** to capture full data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### TIMBRETRON: AWAVENET(CYCLEGAN(CQT(AUDIO))) PIPELINE FOR MUSICAL TIMBRE TRANSFER  \n",
    "[Video](http://www.cs.toronto.edu/~huang/TimbreTron/index.html)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aim to solve problem of **Musical TImbre Transfer** i.e. manipulate timbre from one instrument to match another while preserving other musical content(pitch, rhythym, loudness)\n",
    "- Inspired by **Image Domain** style transfer techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the Constant-Q Transform(CQT) to obtain the TF representation of the audio(log magniutde, no phase)\n",
    "2. CycleGAN for timbre transfer\n",
    "3. Conditional Wavenet Synthesizer\n",
    "\n",
    "![process](fig_03.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why CQT?\n",
    "1. Well suited for timbre due to pitch equivariance\n",
    "2. Simultaneuosly achieves better frequency resolution at low frequencies and high temporal resolution at higher frequencies\n",
    "3. Empirical experiments yielded better performance compared to STFT \n",
    "4. Outperformed traditional representations like MFCC's in environmental sound classifications using CNN's(Husaifah 2017)\n",
    "\n",
    "\n",
    "Issues I felt\n",
    "1. No phase\n",
    "2. Not perfectly invertible(though they claim it is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Dataset</u> - Unrelated collections of different musical instruments(from YouTube, links available in appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Architecture</u> - \n",
    "1. TF representation using log magnitude of CQT.\n",
    "2. **CycleGAN** -\n",
    "    - Unsupervised domain transfer - learn a mapping between two domains **without any paired data**\n",
    "3. Reconstruction from TF representation generated - \n",
    "    - Avoid Griffin Lim as no optimality guarantees\n",
    "    - Hence, use conditional wavenet  to generate waveform\n",
    "4. Major Contributions - \n",
    "    1. **Beam Search** - Possibility of producing low-probability outputs sometimes. To avoid this, search through Wavenet's generations to search output which better matches target CQT\n",
    "    2. **Reverse Generation** - Percussive attacks not modelled correctly at onsets(difficult to determin onset from CQT). Solve by generating samples backwards i.e. in reverse order(Really Cool Hack!!)\n",
    "5. Also proposed architectural modifications for the following - \n",
    "    1. Removing CHeckerboard artifacts\n",
    "    2. Full Spectrogram Discriminator\n",
    "    3. Gradient Penalty\n",
    "    4. Identity Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion - \n",
    "- Evaluation primarily through human evaluation(Amazon Mechanical Turk + feedback form)\n",
    "- Performed an **ablation** study to analyze importance of different architectural changes + importance of certain features\n",
    "- CQT well suited to convolutional architectures mainly due to **pitch equivariance**([What this means in CNN's](https://www.quora.com/What-is-the-difference-between-equivariance-and-invariance-in-Convolution-neural-networks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### WAVENET: A GENERATIVE MODEL FOR RAW AUDIO\n",
    "[Audio Examples](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep neural network to generate **raw audio waveform**(all the above methods generated TF representations which were inverted to obtain waveforms)\n",
    "- Fully **probabilistic + Autoregressive**\n",
    "- Inspired Heavily from previous work by same author i.e. PixelCNN to generate images pixel by pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Models joint probability in the following way - \n",
    "$p(\\bar(x)) = \\Pi_{t=1}^{T}(x_{t}|x_{1},\\dots,x_{t-1})$\n",
    "i.e. each sample depends on all the samples before it.\n",
    "- Use **Dilated Causal Convolutions** to model the above - \n",
    "![causal_convolutions](fig_04.PNG)\n",
    "- **Softmax** vs **GMM** to model conditional distributions\n",
    "- **Gated Activation Units** works as a better non-linearity for audio signals vs ReLU\n",
    "- **Context Stacks** to increase Receptive Field size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditional Wavenets** - \n",
    "- Model conditional distribution given an input h i.e. $p(\\bar(x)|h) = \\Pi_{t=1}^{T}(x_{t}|x_{1},\\dots,x_{t-1},h)$. This is done to produce audio with required characteristics(timbre, pitch)\n",
    "- Done in two ways -\n",
    "1. Global conditioning - Influence output at all time steps\n",
    "2. local conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion - \n",
    "- Lack of long range coherence dut to limited size of Receptive Field\n",
    "- Picks up certain other characteristics in audio such as mimicking accoustics and recording quality(and breathing and mouth movement for speakers in case of speech generation)\n",
    "- Large RF necessary for Music!\n",
    "- Evaluation using **Mean Opinion Scores(MOS)** tests on human evaluated samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders\n",
    "[Website](https://magenta.tensorflow.org/nsynth)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inspired by Wavenet, they deisgn a Wavenet Style autoencoder, that conditions the decoder on **temporal codes** learnt from the raw audio waveform. These capture long term structure without external conditioning.\n",
    "- The model learns a **manifold of embeddings** that allows morphing, interpolations etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Architecture</u>\n",
    "![Nsynth](fig_05.PNG)\n",
    "\n",
    "Thus, instead of external conditioning, the input embeddings work as the conditioning variable, which encodes information about the waveform $p(\\bar(x)) = \\Pi_{t=1}^{T}(x_{t}|x_{1},\\dots,x_{t-1},f(\\bar(x)))$\n",
    "\n",
    "Major Limitation - \n",
    "- Unable to fully capture global context due to memory constraint(open problem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
