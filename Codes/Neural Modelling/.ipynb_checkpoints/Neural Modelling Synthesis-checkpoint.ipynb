{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Modelling Synthesis\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Audio Synthesis\n",
    "***\n",
    "[Link to Audio Examples](https://chrisdonahue.com/wavegan_examples/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WaveGAN** - A first attempt to synthesize audio from raw time-domain waveforms  \n",
    "- Unlike previous naive attempts to simply bootstrap algos by treating spectrograms as images. This offers better performance\n",
    "- Qualitative(Human Judgement) + Quantitative(Inception Score) evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical application in generating sounds - Artist can explore **latent space** of audio, and fine tune variables/parameters to generate desired sound, as opposed to finding his desired sound from dataset. However, audio has high temporal resolution, and thus latent space should encode these high dimensions correctly.  \n",
    "Other work - \n",
    "1. Autoregressive models(Wavenet) -> *Very* slow generation of audio\n",
    "2. GAN's used naively on image spectrograms -> Lossy estimates due to non-invertibility of spectrogram, thus learn an inversion model as well  \n",
    "The authors want to investigate if unsupervised strategies can learn **semantic nodes** implicitly in the high dimensional space rather than being conditioned on them<Discuss more?!>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper - \n",
    "1. Waveform(WaveGAN) + Spectrogram(SpecGAN) strategies for GAN's\n",
    "2. Human Evaluation for sounds + Quantitative evaluation on Speech dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>WaveGAN</u>  \n",
    "Images vs Audio - \n",
    "    - Audio more likely to exhibit periodic structure\n",
    "    - Correlations across large time instants in audio(-> Filter with larger RF!)\n",
    "\n",
    "Architecture - \n",
    "- Modification of Deep Conv GAN\n",
    "    1. Flattened(1D instead of 2D) convolutions\n",
    "    2. Increase stride\n",
    "    3. Rather than original GAN cost(which is unstable due to non-differentiability), use WGAN(modified stable cost function)\n",
    "- Phase Shuffling(artifact prevention, phase invariance)\n",
    "\n",
    "<u>SpecGAN</u>  \n",
    "Phase information is often discarded which prevents inversion of spectrogram\n",
    "\n",
    "Architecture - \n",
    "- Audio ---- [STFT] -> TF Representation ---- [Train DCGAN] -> Obtain samples ---- [Griffin Lim for phase reconstruction] -> Obtain audio from samples\n",
    "\n",
    "Poor performance due to noisiness introduced in Griffin Lim inversion process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Dataset</u>  \n",
    "1. Speech(SC09)\n",
    "2. Sounds(Drum, Bird, Piano, Large Vocab Speech)\n",
    "\n",
    "**Took ~4 days to train!**\n",
    "\n",
    "<u>Evaluation</u>  \n",
    "1. Inception Score - $e^{E_{x}KL(P(y/x)||P(y))}$\n",
    "    - P(y/x) should be low entropy(deterministic){Data generated given class label}\n",
    "    - P(y) should be high entropy(uniform){Data generated across all labels}\n",
    "2. NN comparison - Correct for errors in inception score\n",
    "3. Qualitative Human Judgement - **Amazon Mechanical Turk** to collect/label audio. Digit perceived(SC09) + Sound quality(0-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future Work - \n",
    "- Variable length audio\n",
    "- label conditioning strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Adversarial Generation of Time-Frequency Features with application in audio synthesis\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STFT as Time Frequency(TF) Representation of audio i.e. GAN trained on STFT features. This outperforms models trained on waveform directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Phase of STFT hard to understand and model => Use partial derivatives of phase(local instantaneous frequency).\n",
    "- Phase estimation from magnitude spectrogram(Griffin Lim) unreliable\n",
    "- Inspired from Phaseless Reconstruction + Current State of Art i.e. Gansynth -> TiFGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Math</u>  \n",
    "- STFT maps time-domain signals in to a lower-dimensional subspace of all possible magnitudes\n",
    "- Look at STFT as an operator, and study the conditions for the operator to be perfectly invertible.\n",
    "- For a consistent transformation, assuming that STFT is an analytic function, using the Cauchy Riemann equations, you get a coupled pair of equations which can be solved to obtain the phase simply from the log-Magnitude Spectrogram itself.\n",
    "- Use of **Phase Gradient Heap Integration(PGHI)** to bypass phase instabilitites by providing betted estimates.\n",
    "- The above condition is discretized, and using a new metric **consistency**, the authors judge goodness of TF representation. Consistency evaluated by looking ate projection error $\\hat{e} = |S^{gen} - S^{proj}|^{2}$ where $S^{proj} = ISTFT(STFT(S))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture\n",
    "- Modified DCGAN + preprocessing on signal to enable its input to a GAN\n",
    "\n",
    "Evaluation\n",
    "- Dataset - \n",
    "1) Speech commands dataset\n",
    "2) MUSIC, 25 min of BACH piano recordings\n",
    "- Evaluation using Inception Score and Frechet inception distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion, Future work  -  \n",
    "1. Consistency measure - Computationally cheap measure to assess quality of TF representation  \n",
    "2. Extension - Use logarithmic and perceptual frequency scales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
