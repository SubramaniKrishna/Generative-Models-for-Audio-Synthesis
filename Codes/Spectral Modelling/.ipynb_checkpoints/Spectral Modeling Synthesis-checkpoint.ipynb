{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Modeling Synthesis\n",
    "\n",
    "Papers read - \n",
    "1. A system for sound analysis based on Sinusoidal + stochastic decomposition(Prof. Xavier's thesis)<1,2,3,4>\n",
    "2. Morphing guided by perception<5,6,7,8,12>\n",
    "3. Creative Music<10,11>\n",
    "4. Perception Studies using MDS<9,13,14>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Prof. Xaviers Thesis\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Motivation - To obtain **musically useful** intermediate representation for sound transformations by modelling the spectral characteristics of sound\n",
    "- Underlying Assumption \n",
    "    $$ x = x_{sine} + x_{stochastic}$$\n",
    "    Where, $x_{sine} = \\Sigma_{i} A_{i}[n]sin(\\omega_{i}[n] + \\phi_{i}[n])$ is a sinusoid captured by time varying amplitude, frequency and phase and $x_{stochastic}$ is the stochastic(non-deterministic) component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What constitutes a **good** transformation? \n",
    "- Flexibility(ease of transformation)\n",
    "- Computationally Efficient\n",
    "- Should faithfully reproduce the original sound with as good quality as it can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](fig_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background on some Synthesis Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Historical Background - \n",
    "    1. Tape Recorders\n",
    "    2. Analog tapes(Music Concrete)\n",
    "    3. Digital\n",
    "- Techniques borrowed from Speech Analysis - \n",
    "    1. Vocoder \n",
    "        - Modeling of speech by an excitation waveform(sound source) which is filtered(vocal tract)\n",
    "        - Were able to obtain interesting sound effects(pitch modification, timbre morphing)\n",
    "    2. Linear Predictive Coding \n",
    "        - Linear time varying filtering\n",
    "    3. Phase Vocoder \n",
    "        - Representing signals by the short time phase and amplitude spectrum\n",
    "        - Major motivation to move towards the Short Time Fourier Transform(STFT)\n",
    "- Synthesis Methods - \n",
    "    1. LPC based synthesis \n",
    "        - wide variety of transformations because of the decomposition\n",
    "        - works well when analyzed sounds have **clear formant structure**\n",
    "    2. Analysis based synthesis - \n",
    "        1. Heterodyne filtering \n",
    "            - breaks input waveform into pseudoperiodic segments and then estimates the pitch of each pseudoperiodic segment\n",
    "            - Similar to STFT, analyzes signal at multiple, evenly-spaced time points\n",
    "            - [The Application of Heterodyne Filter Analysis and Linear Predictive Coding\n",
    "using cSound's ADSYN, LPREAD, and LPRESON Opcodes](http://baguyos.tripod.com/DMPST.html)\n",
    "        2. Phase Vocoder\n",
    "            - Manipulate Temporal and Spectral Features independently(decouple them)\n",
    "        3. Formant wave-function synthesis\n",
    "            - Directly modelling the time domain amplitude\n",
    "            - [Time Domain FoF](https://link.springer.com/chapter/10.1007/978-94-009-9091-3_21)\n",
    "            - [Final Project: Formant-Wave-Function Synthesis](https://ccrma.stanford.edu/~mjolsen/220a/fp.html)\n",
    "        4. VOSIM \n",
    "            - model with sinc pulses of variable amplitudes, delays\n",
    "            - [paper](http://www.atiam.ircam.fr/wp-content/uploads/2011/12/AES_JAES_1978_Kaegi_VOSIM.pdf)\n",
    "        5. Wavelet transform\n",
    "            - Wavelets as analysis functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Time Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why perform analysis in the spectral domain?\n",
    "    - Our ear is like a harmonic analyzer, thus spectral analysis mimics the bahaviour of the ear\n",
    "    - Cochlea is likened to a set of narrow band pass filters, thus it performs some kind of FT\n",
    "- How our ear is different?\n",
    "    - Our ear obtains a **log scale spectrum** as opposed to the linear spectra obtained by conventional FT\n",
    "    - Time and Frequency domain masking\n",
    "    - Amplitude perception relative to frequency\n",
    "- [Hearing and Perception](http://artsites.ucsc.edu/ems/music/tech_background/te-03/teces_03.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The STFT equation - \n",
    "    $X_{l}(k) := \\Sigma_{n=0}^{N-1} w(n)x(n+lH)e^{-j\\omega_{k}n}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 important (controllable) parameters - \n",
    "1. Analysis window w(n)\n",
    "    - Determines time vs frequency resolution\n",
    "    - Want narrow main lobe, low side lobe\n",
    "    - For phase detection, constant phase spectrum obtained by using symmetric window\n",
    "2. Hop size H\n",
    "    - Depends on sound characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why move on?\n",
    "- Cannot manipulate sounds easily  \n",
    "\n",
    "Treat this as an intermediate step to obtain a more flexible representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sinusoidal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model a signal as a sum of time varying sinusoids  \n",
    "\\begin{align}\n",
    "&s(t) = \\Sigma_{r=1}^{R} A_{r}(t) cos(\\theta_{r}(t))\\\\\n",
    "&\\theta_{r}(t) = \\int_{0}^{t} \\omega_{\\tau}(\\tau)d \\tau + \\theta_{r}(0) + \\phi_{r}\n",
    "\\end{align}  \n",
    "Here, **R** is the number of sinusoidal components, **$A_{r}(t)$** is the instantaneous amplitude and **$\\theta_{r}(t)$** is the instantaneous phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](fig_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main steps in the parameter extraction are - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spectral Peak Detection - \n",
    "    - Peak detection\n",
    "        - Local maxima in the magnitude spectrum at each time frame\n",
    "        - Filtering the maxima with some threshold measure\n",
    "        - <Optional> For perceptual purposes, use knowledge of equal loudness contours\n",
    "    - Peak interpolation\n",
    "        - Return a better estimate for the frequency than the bin value\n",
    "        - Fit a parabola to the frequency, and use the peak of parabola as estimate \n",
    "The output of this stem is the estimated magnitude, frequency and phase of the prominent peaks in the STFT for each time frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](fig_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Spectral Peak Continuation - \n",
    "    - Map the peaks at the $(n-1)^{th}$ time frame to the $(n)^{th}$ time frame\n",
    "    - Find the peak in the $(n)^{th}$ time frame which is closest in frequency in the previous frame\n",
    "    - Possible approaches - heuristic(rule based), probabilistic(hmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](fig_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the parameters are obtained, the sound is synthesized by generating the sum of sinusoids for each time frame in the following way -  \n",
    "\\begin{align}\n",
    "s^{l}(m) = \\Sigma_{r=1}^{R} \\hat A^{l}_{r}cos(m\\hat \\omega^{l}_{r} + \\hat \\phi^{l}_{r})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sound Effects - Can be easily achieved by playing around with the obtained parameters(scaling, interpolation, filtering etc. ). For ease of manipulation, only the magnitude spectra is used as the ear is mainly sensitive to the spectral magnitude and not the phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why move on?\n",
    "- Difficult to model **noise** with sinsusoids(need a large number)\n",
    "- Because of the lack of noise modelling, the percieved quality is a bit artificial during transformations  \n",
    "    This is motivation to model the noise in the signal as the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic + Residual Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the **deterministic** component different from the previous?\n",
    "- As opposed to selecting any peak in the spectrum(like in the previous case), the deterministic models particularly model the partials in the sound\n",
    "- Thus, each sinusoid is assumed to model a **quasi-sinusoidal** component(piecewise linear amplitude and frequency variation) as opposed to any kind of sound   \n",
    "\n",
    "The **Residual** in this case is defined $ x_{original} - x_{deterministic} $. It usually models the energy that does not go into vibrations, or any component that is not inherently sinusoidal in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal in this case is modelled as - \n",
    "\\begin{align}\n",
    "s(t) = \\Sigma_{r=1}^{R} A_{r}(t) cos(\\theta_{r}(t)) + e(t)\\\\\n",
    "\\end{align}\n",
    "Here, e(t) is the residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](fig_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the steps in extracting the parameters for the deterministic model are the same as the previous model. But, since the sinusoids are restricted to be partials only here, there is a modification in the **Spectral Peak Continuation** process. Using a heuristic(rule based) and some prior knowledge about the nature of the sound(harmonic, frequency range etc. ), an algorithm is proposed which tracks only the clear and stable partials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deterministic components are synthesized using the parameters obtained. The residual is obtained by subtracting this deterministic signal from the original signal.  \n",
    "An easier alternative is to subtract the frequency spectra of the two signals and ignoring the phase(perceptually unimportant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why move on ? \n",
    "- Residual is not flexible for performing transformations  \n",
    "\n",
    "This motivates to further study the residual, and approximate it with a model that can be easily played around with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic + Stochastic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from the previous models - \n",
    "- Not necessary to preserve phase\n",
    "- Can model the residual as some kind of stochastic signal  \n",
    "\n",
    "Modelling the residual as a stochastic signal helps in easily transforming the signal  \n",
    "\n",
    "![title](fig_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representation obtained is similar to the previous case, just that the residual e(t) is modelled as a stochastic signal, thus allowing to write as the action of a Linear Time Variant system on white noise.  \n",
    "\\begin{align}\n",
    "\\hat e(t) = \\int_{0}^{t}h(t,t-\\tau)u(\\tau)d \\tau\n",
    "\\end{align}\n",
    "Here, u(t) is white noise and h(t,t') is the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](fig_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deterministic component is calculated in the same way as the previous. The parameters are set in such a way as to extract the partials as accurately as possible(to prevent them from appearing in the residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we assume the residual to be a stochastic signal, it is characterized by its amplitude and frequency.   \n",
    "To obtain the general shape of the residual spectrum, we approximate the envelope of the residual spectrum, which is obtained by subracting the deterministic spectra from the original spectra. This is because only the shape of the envelope contributes to the sound characteristics. The envelope is approximated by **curve fitting** or **LPC**.   \n",
    "Once the envelope is obtained, we generate the stochastic signal by using this as our amplitude and generate random numbers as phase\n",
    "\\begin{align}\n",
    "\\hat e(t) = IFT(A(k)e^{j \\Theta(k)})\n",
    "\\end{align}\n",
    "Here, A(k) is the envelope, and $\\Theta(k)$ is the phase(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations - Can be separately applied to the deterministic and stochastic components. \n",
    "- Deterministic - Similar transformations like before\n",
    "- Stochastic - Envelope shaping, filtering etc.   \n",
    "\n",
    "![fig](fig_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of sound effects using the above model (Refer 4.pdf)\n",
    "1. Filtering\n",
    "2. Pitch Scaling, transposition and discretization\n",
    "3. Vibrato, tremolo\n",
    "4. Spectral shape shifting\n",
    "5. Gender changing\n",
    "6. Harmonizing\n",
    "7. Hoarseness\n",
    "8. Morphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Musical Instrument Sound Morphing Guided by Perceptually Motivated Features\n",
    "***\n",
    "For sound examples, visit [this page](http://recherche.ircam.fr/anasyn/caetano/overview.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is **Morphing**?  \n",
    "- Blurring Distinction between **Source** and **Target**\n",
    "- Somewhat like creating **hybrid** musical instruments  \n",
    "- Would like to ideally perform **Perceptually Linear** transformations\n",
    "- The morphed sound should not simply sound like a mixture of sounds(the ear can distinguish in such cases). It should rather sound like a single **entity**  \n",
    "\n",
    "How is it done?\n",
    "\n",
    "- Obtain some kind of reprsentation of the sound, and then have an interpolation function that gradually interpolates these representations from one sound to the other.  \n",
    "- Control the whole morphing process(algorithmically and perceptually) with a single coefficient $\\alpha$ , the interpolation factor\n",
    "- You would ideally want to vary the interpolate the parameters so that the morphed sound vary **perceptually linearly**\n",
    "\n",
    "In this work, the authors have proposed to seek sound parameters that favor Perceptually Linear transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work done previously  \n",
    "- Mostly interpolate parameters/features without caring much about the perceptual impact\n",
    "![Classical Morphing](fig_9.PNG)  \n",
    "\n",
    "What are parameters,features?\n",
    "- Parameters - Coefficients obtained from sound analysis models(can resynthesize sound from them)\n",
    "- Features - Particular aspects of sound  \n",
    "\n",
    "Methods Used - \n",
    "1. Parameter interpolation using Wigner Distributions(Time Frequency)  \n",
    "2. GMM models for parameter interpolation  \n",
    "3. **Model Sounds as dynamical systems with ANN**  \n",
    "4. Discrete Wavelet Transform(DWT) + Singular Value Decomposition(SVD)  \n",
    "\n",
    "The above don't consider perceptual factors and suggest suggest interpolation strategies with better perceptual corelations, like the ones below\n",
    "1. Dynamic Frequncy Warping(DFW) to morph spectral envelopes\n",
    "2. Multi Dimensional Scaling(MDS)\n",
    "\n",
    "One important thing to consider in all the above cases is the need for the sound to be **temporally alligned**, or else some kind of smearing might occur, thus making the resultant sound artificial to hear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work, as opposed to interpolating parameters directly, the authors propose to first obtain relevant features from the parameters(which might have a more perceptual meaning than the parameters themselves), and then interpolate these features itself.  \n",
    "![Authors Proposed Idea](fig_10.PNG)\n",
    "However, obtaining parameters from features is difficult(It is not a one-one transformation!). Thus, instead of this approach, the authors propose to use parameters for whom the interpolated sounds features are close to the interpolated feature values(suitable evaluation scheme suggested, use parameters -> feauture values vary linearly when interpolating linearly) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features the authors use in this work are obtained by finding **accoustic correlates of Timbre Spaces using MDS**(Essentially trying to mathematically describe the Timbre Space). The features are both temporal and spectral.  \n",
    "\n",
    "Temporal  \n",
    "1. log attack time\n",
    "2. temporal centroid  \n",
    "\n",
    "Spectral\n",
    "1. spectral centroid\n",
    "2. spectral spread\n",
    "3. spectral skewness\n",
    "4. spectral kurtosis\n",
    "\n",
    "The authors proposed model - \n",
    "![Authors Proposed Model](fig_11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of Parameter - \n",
    "1. Temporal Segmentation - Segment into ADSR\n",
    "2. Temporal Allignment - Boundaries should coincide\n",
    "3. Temporal Envelope Extraction - True Amplitude Envelope(TAE) based on cepstral smoothing\n",
    "4. Sinusoidal + Residual Model - To obtain the parameters\n",
    "5. Source Filter Model - \n",
    "\n",
    "Morphing - \n",
    "1. Spectral Envelope Morphing - Shift in frequency peaks smoothly\n",
    "2. Interpolation of partial frequencies\n",
    "3. Temporal Envelope Morphing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "- Vienna Symphonic Library\n",
    "- Listening Test \n",
    "    - Judge several Characteristics for each morph value\n",
    "    - Complicated and very subjective\n",
    "- Proposed Objective error function(assuming linearity, essentially the MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Spectral Envelope Estimation and Representation for Sound Analysis–Synthesis\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is it - Envelope of magnitude of Short Time Spectrum(STS)\n",
    "1. Speak about **Spectral Envelopes** in sound analysis and synthesis.\n",
    "2. Linked to perception, and how they capture **important properties** of sound.\n",
    "3. Challenges - Not east to estimate and represent them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you want - \n",
    "    1. Envelope fit - Links the peaks of partials\n",
    "    2. Smoothness - should not oscillate wildly, just give a rough idea of the shape\n",
    "    3. Adaptation to fast spectral variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods of Estimation - \n",
    "    1. Linear predictive coding\n",
    "    2. Cepstrum\n",
    "    3. Discrete cepstrum\n",
    "\n",
    "What is wanted - \n",
    "    1. Precision\n",
    "    2. Stability(like BIBO, small changes in data should give small changes in output)\n",
    "    3. Locality in frequency - parameters should cause local, not global changes\n",
    "    4. Flexibility and ease of manipulations - Should be easily manipulable with tunable parameters\n",
    "    5. Speed of synthesis \n",
    "    6. Space in memory\n",
    "    7. Manual input - manual tuning/control\n",
    "    \n",
    "Proposed Representations - \n",
    "    1. Filter coefficients\n",
    "    2. Sampled representation\n",
    "    3. Geomteric representation\n",
    "    4. Formants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be done with them - \n",
    "    1. Influence timbre\n",
    "    2. Enhance musical expressivity\n",
    "\n",
    "Link to webpage describing the above in detail - [link](http://recherche.ircam.fr/anasyn/schwarz/publications/icmc1999/se99-poster.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### AUTOMATIC TIMBRAL MORPHING OF MUSICAL INSTRUMENT SOUNDS BY HIGH-LEVEL DESCRIPTORS\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precursor to the second paper above.  \n",
    "Speak about the importance of taking into account perception while morphing(go through paper highlights, most work is extension of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### AUTOMATIC AUDIO MORPHING\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic morphing from one sound to another\n",
    "- Sound represented in a multi-dimensional space\n",
    "- Axes represent features that **perceptually represent** the sound, in this case spectral shape and pitch\n",
    "- The axes are assumed to be orthogonal i.e. each axis can be transformed independent of the others\n",
    "- Morphing essentially represents interpolation in this space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One sound should **smoothly** change into another sound. The process is described below in the figure - \n",
    "![fig](fig_12.PNG)\n",
    "1. A sound representation in the 'space'(in this case pitch and envelope) is obtained.\n",
    "2. Matching(allignment) to allign relevant features\n",
    "3. Interpolation, followed by reconstruction\n",
    "\n",
    "Spectrograms are used to encode the two axes(smooth spectrogram using MFCC's, followed by a pitch spectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The morphing - Linear interpolation between the matched features\n",
    "![morphing](fig_13.PNG)\n",
    "The signal is linearly interpolated in time from features of signal 1 to signal 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work - \n",
    "1. Better repesentations\n",
    "2. Better matching techniques\n",
    "3. **Perceptually optimal interpolation functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Creative Music(Work by Tristan Jehan at MIT Media Lab)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What has he done - In his Masters thesis<11>, He has made what he calls a **hyperviolin** - An instrument whose sounds/timbre can be modified in realtime by modifiying parameters. Then, in his PhD thesis<10>, he creates a system which can basically **create** music from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Msc Thesis - Audio Driven Timbre Generator\n",
    "Salient points - \n",
    "- **Little to no digital synthesis technology for non keyboard instruments**{Potential Work area}\n",
    "- Limitations in current systems are - \n",
    "    1. Quality\n",
    "    2. Control over synthesis\n",
    "    3. Instrument specific\n",
    "- Hyperviolin - \n",
    "    1. Takes musical performance data(audio + gesture)\n",
    "    2. Processing(can be in realtime)\n",
    "    3. Generation/Synthesis\n",
    "- They model **Physical Sound**, not the perceptual features(What they call **Timbre Model**)\n",
    "- **Almost no work done on perceptually-controlled sound synthesis**{Potential Work area}\n",
    "- Future work - \n",
    "    1. Algorithms that extract **better and new** perceptual features\n",
    "    2. Study the **evolution of parameters** rather than instantaneuos parameters i.e. take into account how parameters evolve in the piece and transform accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "PhD Thesis - Creating Music by Listening\n",
    "- High degree of abstraction between sampled audio and mental perception of it. Authors propose to bridge this gap by modeling human perception and learning of music.\n",
    "- Composing new music by **recycling** pre-existing music.\n",
    "- Thinks of all possible audio signals as a space. Music is a very small subset with some structure in this space. The authors want to make an **intelligent search** in this space to **re-discover** music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting points - \n",
    "- Perceptual Synthesis Engine(MSc thesis work, SMS + Perception) -> Decompose audio to parameters(frequency, amplitude) and their perceptual correlates(instantaneuos pitch, loudness, brightness), then learn relation between two data\n",
    "- Use other metadata(besides audio) like acoustic, cultural editorial for retrieval{Can we do for synthesis, study what kinds of metadata is available}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Music Cognition Machine**\n",
    "![Proposed Framework](fig_14.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
